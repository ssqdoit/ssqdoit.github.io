import{_ as i,c as a,o as s,a5 as t}from"./chunks/framework.DHgPyqoO.js";const k=JSON.parse('{"title":"引言","description":"","frontmatter":{},"headers":[],"relativePath":"daily/2025/什么是知识蒸馏（Knowledge Distillation）.md","filePath":"daily/2025/什么是知识蒸馏（Knowledge Distillation）.md","lastUpdated":null}'),l={name:"daily/2025/什么是知识蒸馏（Knowledge Distillation）.md"},e=t('<h1 id="引言" tabindex="-1">引言 <a class="header-anchor" href="#引言" aria-label="Permalink to &quot;引言&quot;">​</a></h1><p>DeepSeek 推理模型的爆火，也成了茶余饭后的话题。前段时间和同事一起吃饭便聊到了 LLM 知识蒸馏技术，但我们都是非专业背景下对这个知识蒸馏技术其实挺陌生，通过学习并记录下来。</p><h1 id="蒸馏和微调的区别" tabindex="-1">蒸馏和微调的区别 <a class="header-anchor" href="#蒸馏和微调的区别" aria-label="Permalink to &quot;蒸馏和微调的区别&quot;">​</a></h1><h3 id="微调-fine-tuning" tabindex="-1">微调（Fine-tuning） <a class="header-anchor" href="#微调-fine-tuning" aria-label="Permalink to &quot;微调（Fine-tuning）&quot;">​</a></h3><p>微调是指在预训练模型的基础上，通过少量数据进一步训练，以适应特定任务。具体步骤如下：</p><ul><li><strong>预训练模型</strong>：在大规模数据集上训练好的模型。</li><li><strong>任务特定数据</strong>：针对特定任务的小规模数据集。</li><li><strong>调整参数</strong>：在预训练模型的基础上，使用任务特定数据进行少量训练，调整模型参数。</li></ul><p>微调的目的是利用预训练模型的通用特征，快速适应新任务。</p><h3 id="蒸馏-distillation" tabindex="-1">蒸馏（Distillation） <a class="header-anchor" href="#蒸馏-distillation" aria-label="Permalink to &quot;蒸馏（Distillation）&quot;">​</a></h3><p>蒸馏是一种模型压缩技术，用于将大型模型（教师模型）的知识转移到小型模型（学生模型）上。具体步骤如下：</p><ul><li><strong>教师模型</strong>：一个复杂且性能较好的模型。</li><li><strong>学生模型</strong>：一个更小、更简单的模型。</li><li><strong>知识转移</strong>：通过让学生模型模仿教师模型的输出（如软标签），实现知识传递。</li></ul><p>蒸馏的目的是在保持较高性能的同时，减少模型的计算和存储需求。</p><h3 id="主要区别" tabindex="-1">主要区别 <a class="header-anchor" href="#主要区别" aria-label="Permalink to &quot;主要区别&quot;">​</a></h3><ul><li><strong>目的</strong>：蒸馏用于模型压缩，同时保留较高的性能；微调用于适应特定任务需求。</li><li><strong>方法</strong>：蒸馏通过知识转移，在优化学生模型时是不需要引入新的参数的；微调一般是有监督学习，微调通过使用带标签的数据调整预训练模型，使其更好地适应特定任务或数据集（例如知识库QA）。</li><li><strong>应用场景</strong>：蒸馏适用于资源受限的环境，微调适用于特定任务。</li></ul><h2 id="什么是蒸馏" tabindex="-1">什么是蒸馏 <a class="header-anchor" href="#什么是蒸馏" aria-label="Permalink to &quot;什么是蒸馏&quot;">​</a></h2><h3 id="核心思想" tabindex="-1">核心思想 <a class="header-anchor" href="#核心思想" aria-label="Permalink to &quot;核心思想&quot;">​</a></h3><p>知识蒸馏的目标是将一个复杂的、性能较好的模型（教师模型）的知识转移到一个更小、更简单的模型（学生模型）中。这里的&quot;知识&quot;指的是教师模型对输入数据的输出分布（即<code>软标签</code>，Soft Labels），而不仅仅是<code>硬标签</code>（Hard Labels，如分类任务中的类别标签）。</p><h3 id="什么是标签" tabindex="-1">什么是标签 <a class="header-anchor" href="#什么是标签" aria-label="Permalink to &quot;什么是标签&quot;">​</a></h3><p><strong>1. 硬标签（Hard Labels）</strong></p><ul><li><strong>硬标签</strong>是传统监督学习中的标准标签形式，通常为<strong>离散的、确定性的类别标签</strong>。</li><li>在分类任务中，硬标签一般以 <strong>One-Hot编码</strong> 表示。例如，在图片分类任务中，例如识别图片中的动物是猫、狗、鸟，真实识别为猫，则硬标签为 <code>[1, 0, 0]</code>。</li></ul><p><strong>特点</strong></p><ul><li><strong>确定性</strong>：明确指定唯一正确的类别。</li><li><strong>信息量少</strong>：仅保留最终分类结果，不反映模型对不同类别的置信度或类别间的关系。</li><li><strong>训练目标直接</strong>：直接优化模型输出与真实标签的匹配度（如<code>交叉熵损失</code>）。</li></ul><p><strong>2. 软标签（Soft Labels）</strong></p><ul><li><strong>软标签</strong>是教师模型对输入数据预测的<strong>概率分布</strong>，通过 Softmax 函数生成。</li><li>例如，教师模型对识别图片中的动物是猫、狗、鸟概率分别为 <code>[0.6, 0.3, 0.1]</code>（对每个动物的置信度）。</li></ul><p><strong>特点</strong></p><ul><li><strong>概率性</strong>：反映教师模型对各个类别的置信度，包含更多信息（如类别间相似性）。</li><li><strong>知识丰富性</strong>：包含教师模型的泛化能力和对&quot;模糊样本&quot;的决策逻辑（例如，某样本属于猫的概率为 0.6，狗的概率为 0.3，鸟的概率为 0.1）。</li></ul><h3 id="模型输出的概率是什么意思" tabindex="-1">模型输出的概率是什么意思 <a class="header-anchor" href="#模型输出的概率是什么意思" aria-label="Permalink to &quot;模型输出的概率是什么意思&quot;">​</a></h3><p>例如上面描述的</p><blockquote><p>猫、狗、鸟概率分别为 <code>[0.6, 0.3, 0.1]</code></p></blockquote><p>这个输出是一个<strong>概率分布</strong>，表示模型对输入数据的预测结果。具体含义取决于任务类型：</p><h4 id="如果是分类任务" tabindex="-1">如果是分类任务： <a class="header-anchor" href="#如果是分类任务" aria-label="Permalink to &quot;如果是分类任务：&quot;">​</a></h4><ul><li><p>假设任务是将输入分为 3 个类别（例如猫、狗、鸟）。</p></li><li><p><code>[0.6, 0.3, 0.1]</code> 表示模型认为输入属于：</p><ul><li>类别 1 的概率是 60%（例如猫）。</li><li>类别 2 的概率是 30%（例如狗）。</li><li>类别 3 的概率是 10%（例如鸟）。</li></ul></li></ul><h4 id="如果是文生文任务-如我们经常使用的-chatgpt-对话生成" tabindex="-1">如果是文生文任务（如我们经常使用的 <code>chatgpt</code> 对话生成）： <a class="header-anchor" href="#如果是文生文任务-如我们经常使用的-chatgpt-对话生成" aria-label="Permalink to &quot;如果是文生文任务（如我们经常使用的 `chatgpt` 对话生成）：&quot;">​</a></h4><ul><li><p>文生文任务是一个<strong>序列生成任务</strong>，模型需要逐词（或逐 token）生成输出。</p></li><li><p>在每一步生成时，模型会输出一个概率分布，表示下一个词（或 token）的可能性。</p></li><li><p>例如，假设词汇表有 3 个词（&quot;你好&quot;、&quot;再见&quot;、&quot;谢谢&quot;），模型输出的 <code>[0.756, 0.178, 0.066]</code> 表示：</p><ul><li>下一个词是&quot;你好&quot;的概率是 75.6%。</li><li>下一个词是&quot;再见&quot;的概率是 17.8%。</li><li>下一个词是&quot;谢谢&quot;的概率是 6.6%。</li></ul></li></ul><h3 id="知识蒸馏的大致步骤" tabindex="-1">知识蒸馏的大致步骤 <a class="header-anchor" href="#知识蒸馏的大致步骤" aria-label="Permalink to &quot;知识蒸馏的大致步骤&quot;">​</a></h3><h4 id="_1-训练教师模型" tabindex="-1">1. 训练教师模型 <a class="header-anchor" href="#_1-训练教师模型" aria-label="Permalink to &quot;1. 训练教师模型&quot;">​</a></h4><ul><li>教师模型是一个大型的、复杂的模型（例如 DeepSeek-R1）。</li><li>在训练集上训练教师模型，直到其达到较高的性能。</li><li>教师模型的输出不仅包括最终的预测类别（硬标签），还包括每个类别的概率分布（<code>软标签</code>）。</li></ul><h4 id="_2-生成软标签" tabindex="-1">2. 生成软标签 <a class="header-anchor" href="#_2-生成软标签" aria-label="Permalink to &quot;2. 生成软标签&quot;">​</a></h4><ul><li>对于每个输入数据，教师模型会输出一个概率分布（软标签）。例如，在识别图片是猫、狗、鸟的分类任务中，教师模型可能会输出：</li></ul><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>这表示模型认为输入的图片 60% 的概率属于猫，30% 的概率属于狗，10% 的概率属于鸟。</p><ul><li>软标签包含了更多的信息，比如类别之间的相对关系（例如：猫 和 狗 更相似）。</li></ul><h4 id="_3-温度参数-temperature" tabindex="-1">3. 温度参数（Temperature） <a class="header-anchor" href="#_3-温度参数-temperature" aria-label="Permalink to &quot;3. 温度参数（Temperature）&quot;">​</a></h4><ul><li>在生成软标签时，会引入一个**温度参数（Temperature, T）**来平滑概率分布。</li><li>温度参数的作用是调整教师模型输出的软标签的&quot;软度&quot;。较高的温度会使概率分布更加平滑(&quot;模糊&quot;、&quot;均匀&quot;)，类别之间的概率差距缩小，模型能够看到更多类别的&quot;相似性&quot;，从而让学生模型更容易学习到类别之间的关系。较低的温度则正好相反。（ps: 较小的温度会使得模型的输出更加稳定）</li><li>温度参数的公式如下：</li></ul><p><img src="https://feifan.iflytek.com/oss/2025/2/20/10/15/44/129/47418/556.png" alt="img"></p><p>其中，<em>zi</em> 是教师模型输出的 logits，<em>T</em> 是温度参数。</p><p>当 <em>T</em>=1 时，Softmax 的输出是原始的概率分布。</p><p>当 <em>T</em>&gt;1 时，Softmax 的输出会更加平滑，概率分布会更加均匀。</p><p>当 <em>T</em>&lt;1 时，Softmax 的输出会更加尖锐，概率分布会更加集中。</p><h4 id="_4-计算损失函数" tabindex="-1">4.计算损失函数 <a class="header-anchor" href="#_4-计算损失函数" aria-label="Permalink to &quot;4.计算损失函数&quot;">​</a></h4><p>在知识蒸馏中，学生模型的训练通常结合了两部分损失：</p><ul><li><strong>蒸馏损失（Distillation Loss）</strong><br> 用于衡量学生模型输出与教师模型软标签之间的差异。常见的方法是使用 <code>KL 散度</code>（Kullback-Leibler Divergence）：</li><li><strong>监督损失（Supervised Loss）</strong><br> 当训练数据包含真实标签时，可以同时使用<code>交叉熵</code>损失来直接监督学生模型，使其输出接近真实标签（硬标签）：</li></ul><blockquote><p>ps: 这里不做太多的介绍了，一方面篇幅有限，另方面本身是小白也在学习中...</p></blockquote><h4 id="_5-训练学生模型" tabindex="-1">5. 训练学生模型 <a class="header-anchor" href="#_5-训练学生模型" aria-label="Permalink to &quot;5. 训练学生模型&quot;">​</a></h4><blockquote><p>学生模型是一个更小、更简单的模型（例如 DeepSeek-R1-32B），学生模型的目标是模仿教师模型的输出（<code>软标签</code>），而不是直接学习<code>硬标签</code>。</p></blockquote><ul><li>通过反向<code>传播算法</code>，计算损失函数对学生模型参数的梯度。</li><li>使用梯度下降法更新学生模型的参数，使得损失函数逐渐减小。</li><li>经过多次迭代，学生模型的输出分布会越来越接近教师模型的输出分布。</li></ul><p>反向传播算法（Backpropagation）是训练神经网络的核心算法之一。它的作用是计算损失函数对模型参数的梯度，然后利用这些梯度来更新模型的参数。</p><blockquote><p>反向传播的步骤:</p><ol><li><p><strong>前向传播（Forward Pass）</strong>：</p><ul><li>输入数据通过学生模型，计算模型的输出。</li><li>例如，输入一张猫的图片，学生模型输出 <code>[0.756, 0.178, 0.066]</code>（表示属于猫、狗、鸟的概率）。</li></ul></li><li><p><strong>计算损失（Loss Calculation）</strong>：</p><ul><li>将学生模型的输出与目标（教师模型的软标签 <code>[0.622, 0.245, 0.133]</code>）进行比较，计算损失值（如 KL 散度）。</li></ul></li><li><p><strong>反向传播（Backward Pass）</strong>：</p><ul><li>从损失函数开始，沿着计算图（Computation Graph）反向传播，计算损失函数对每个参数的梯度。</li><li>梯度表示的是：如果稍微调整某个参数，损失函数会如何变化。</li></ul></li><li><p><strong>更新参数（Parameter Update）</strong>：</p><ul><li>使用梯度下降法（Gradient Descent）或其他优化算法，根据梯度更新模型的参数。</li></ul></li></ol></blockquote><h3 id="知识蒸馏的示例" tabindex="-1">知识蒸馏的示例 <a class="header-anchor" href="#知识蒸馏的示例" aria-label="Permalink to &quot;知识蒸馏的示例&quot;">​</a></h3><p>假设我们有一个图片识别分类任务，类别数为3，分别是猫、狗、鸟的概率。</p><h4 id="_1-教师模型的输出-未经过温度调整" tabindex="-1"><strong>1.教师模型的输出（未经过温度调整）</strong>： <a class="header-anchor" href="#_1-教师模型的输出-未经过温度调整" aria-label="Permalink to &quot;**1.教师模型的输出（未经过温度调整）**：&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">logits </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5.0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2.0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1.0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p><strong>Logits</strong> 是模型输出的原始分数，表明三个类别的得分情况（此时不具备概率的属性）</p><p><strong>Softmax</strong> 公式：</p><p>Softmax 是一种数学函数，用于多分类任务中。它的作用是将一组数值（模型输出的 logits）转换为概率分布。具体来说：</p><ul><li>输入：一组数值（logits），例如 <code>[5.0, 2.0, 1.0]</code></li><li>输出：一组概率值，所有值的和为 1，例如 <code>[0.936, 0.047, 0.017]</code></li></ul><p><img src="https://feifan.iflytek.com/oss/2025/2/20/10/16/22/437/25783/317.png" alt="img"></p><p>经过 <strong>Softmax</strong> 计算将 logits 转化为概率分布后：</p><p><img src="https://feifan.iflytek.com/oss/2025/2/20/10/16/57/174/47483/96.png" alt="img"></p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">softmax(logits) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.936</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.047</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.017</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>这表明教师模型输出概率分别为：</p><blockquote><p>猫的概率为： 0.936</p><p>狗的概率为： 0.047</p><p>鸟的概率为： 0.017</p></blockquote><h4 id="_2-引入温度参数-t-2" tabindex="-1"><strong>2.引入温度参数（T=2）</strong>： <a class="header-anchor" href="#_2-引入温度参数-t-2" aria-label="Permalink to &quot;**2.引入温度参数（T=2）**：&quot;">​</a></h4><p>引入温度后 <strong>Softmax</strong> 公式：</p><p><img src="https://feifan.iflytek.com/oss/2025/2/20/10/17/33/695/47414/178.png" alt="img"></p><p>使用温度参数计算后的概率分布：</p><p><img src="https://feifan.iflytek.com/oss/2025/2/20/10/17/59/112/47418/876.png" alt="img"></p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">softmax(logits </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> T) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.736</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.164</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.100</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>可以看到，温度参数使概率分布更加平滑。</p><p><strong>3.学生模型的目标</strong>：</p><p>通过计算损失函数对学生模型参数的梯度，更新学生模型已有的参数（如权重和偏置），通过不断更新参数</p><p>让学生模型的输出应该尽量接近 <code>[0.736, 0.164, 0.100]</code></p><p>这样学生模型就习得了教师模型的性能</p><h2 id="总结" tabindex="-1">总结 <a class="header-anchor" href="#总结" aria-label="Permalink to &quot;总结&quot;">​</a></h2><p>知识蒸馏技术作为一种有效的模型压缩方法，能够在保留较高性能的同时，大幅减少模型的计算和存储需求。这使得蒸馏在资源受限的环境中，尤其是在边缘设备和移动端应用中，具有重要的应用价值。</p><p>像 DeepSeek-R1 的满血版蒸馏出 32B、14B 的小模型一样，让个人电脑上运行接近大模型的性能成为了一种可能。</p><h2 id="其他" tabindex="-1">其他 <a class="header-anchor" href="#其他" aria-label="Permalink to &quot;其他&quot;">​</a></h2><ol><li>蒸馏是一种模型压缩技术，常见的模型压缩技术还有 量化、剪枝等</li><li>这里未对学生模型中涉及如何反向传播计算梯度以及损失函数如何预测效果等，后面继续更新。</li></ol>',87),o=[e];function n(r,p,h,d,c,u){return s(),a("div",null,o)}const m=i(l,[["render",n]]);export{k as __pageData,m as default};
